Epoch #1: 10240it [00:08, 1138.90it/s, env_step=10240, len=124, loss=0.121, loss/clip=-0.004, loss/ent=3.101, loss/vf=0.626, n/ep=2, n/st=256, rew=-65.99]                                                              
Epoch #1: test_reward: -58.780711 ± 40.818835, best_reward: -58.780711 ± 40.818835 in #1
Epoch #2: 10240it [00:07, 1428.25it/s, env_step=20480, len=123, loss=0.058, loss/clip=-0.008, loss/ent=2.837, loss/vf=0.376, n/ep=1, n/st=256, rew=-68.86]                                                              
Epoch #2: test_reward: -61.424616 ± 38.247394, best_reward: -58.780711 ± 40.818835 in #1
Epoch #3: 10240it [00:07, 1444.96it/s, env_step=30720, len=91, loss=0.055, loss/clip=-0.004, loss/ent=2.810, loss/vf=0.349, n/ep=1, n/st=256, rew=-59.84]                                                               
Epoch #3: test_reward: -71.894475 ± 7.884593, best_reward: -58.780711 ± 40.818835 in #1
Epoch #4: 10240it [00:07, 1424.65it/s, env_step=40960, len=326, loss=0.047, loss/clip=-0.003, loss/ent=2.757, loss/vf=0.313, n/ep=1, n/st=256, rew=-108.43]                                                             
Epoch #4: test_reward: -65.790090 ± 3.805370, best_reward: -58.780711 ± 40.818835 in #1
Epoch #5: 10240it [00:07, 1448.79it/s, env_step=51200, len=130, loss=0.046, loss/clip=-0.004, loss/ent=2.891, loss/vf=0.314, n/ep=2, n/st=256, rew=-70.57]                                                              
Epoch #5: test_reward: -65.480514 ± 40.500295, best_reward: -58.780711 ± 40.818835 in #1
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 5
