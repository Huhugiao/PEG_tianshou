Epoch #1: 10240it [00:08, 1234.66it/s, env_step=10240, len=135, loss=0.190, loss/clip=-0.001, loss/ent=3.173, loss/vf=0.890, n/ep=4, n/st=256, rew=-44.45]                                                              
Epoch #1: test_reward: -70.682206 ± 6.114663, best_reward: -56.004927 ± 33.605408 in #0
Epoch #2: 10240it [00:06, 1568.01it/s, env_step=20480, len=191, loss=0.061, loss/clip=-0.002, loss/ent=3.157, loss/vf=0.379, n/ep=1, n/st=256, rew=-81.46]                                                              
Epoch #2: test_reward: -70.219266 ± 9.352773, best_reward: -56.004927 ± 33.605408 in #0
Epoch #3: 10240it [00:06, 1553.12it/s, env_step=30720, len=83, loss=0.162, loss/clip=-0.001, loss/ent=3.167, loss/vf=0.779, n/ep=1, n/st=256, rew=-61.00]                                                               
Epoch #3: test_reward: -66.800012 ± 4.819165, best_reward: -56.004927 ± 33.605408 in #0
Epoch #4: 10240it [00:06, 1531.33it/s, env_step=40960, len=137, loss=0.083, loss/clip=-0.002, loss/ent=3.144, loss/vf=0.465, n/ep=3, n/st=256, rew=-72.17]                                                              
Epoch #4: test_reward: -70.092486 ± 3.891423, best_reward: -56.004927 ± 33.605408 in #0
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 4
