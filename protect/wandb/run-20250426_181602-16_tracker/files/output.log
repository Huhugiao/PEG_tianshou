Epoch #1: 10240it [00:12, 848.27it/s, env_step=10240, len=224, loss=0.135, loss/clip=-0.004, loss/ent=3.103, loss/vf=0.679, n/ep=3, n/st=256, rew=-89.59]                                                               
Epoch #1: test_reward: -76.024782 ± 3.207492, best_reward: -53.655710 ± 34.627056 in #0
Epoch #2: 10240it [00:09, 1029.13it/s, env_step=20480, len=200, loss=0.079, loss/clip=-0.010, loss/ent=2.479, loss/vf=0.452, n/ep=2, n/st=256, rew=-84.71]                                                              
Epoch #2: test_reward: -78.223702 ± 3.712322, best_reward: -53.655710 ± 34.627056 in #0
Epoch #3: 10240it [00:09, 1070.51it/s, env_step=30720, len=179, loss=0.079, loss/clip=-0.004, loss/ent=2.910, loss/vf=0.448, n/ep=0, n/st=256, rew=-77.30]                                                              
Epoch #3: test_reward: -77.524902 ± 14.242056, best_reward: -53.655710 ± 34.627056 in #0
Epoch #4: 10240it [00:08, 1147.73it/s, env_step=40960, len=164, loss=0.055, loss/clip=-0.003, loss/ent=3.084, loss/vf=0.356, n/ep=2, n/st=256, rew=-75.54]                                                              
Epoch #4: test_reward: -57.033919 ± 33.576651, best_reward: -53.655710 ± 34.627056 in #0
Epoch #5: 10240it [00:08, 1155.91it/s, env_step=51200, len=116, loss=0.099, loss/clip=-0.003, loss/ent=2.985, loss/vf=0.529, n/ep=0, n/st=256, rew=-65.29]                                                              
Epoch #5: test_reward: -68.572186 ± 5.991179, best_reward: -53.655710 ± 34.627056 in #0
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 5
