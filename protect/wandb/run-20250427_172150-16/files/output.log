Epoch #1: 10240it [00:09, 1076.71it/s, env_step=10240, len=179, loss=-0.032, loss/clip=-0.006, loss/ent=3.114, loss/vf=0.022, n/ep=4, n/st=256, rew=-152.44]
Epoch #1: test_reward: -125.940501 Â± 32.654074, best_reward: -105.504781 Â± 48.703145 in #0
Epoch #2: 10240it [00:07, 1335.08it/s, env_step=20480, len=118, loss=-0.038, loss/clip=-0.011, loss/ent=2.799, loss/vf=0.001, n/ep=3, n/st=256, rew=-116.16]
Epoch #2: test_reward: -114.755133 Â± 12.672983, best_reward: -105.504781 Â± 48.703145 in #0
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 4
Loading agent under protect_vs_invade_adjusting_22
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #2: 10240it [00:09, 1076.68it/s, env_step=20480, len=547, loss=-0.028, loss/clip=-0.006, loss/ent=2.587, loss/vf=0.017, n/ep=0, n/st=256, rew=92.95]
Epoch #2: test_reward: 107.240348 Â± 17.063078, best_reward: 107.240348 Â± 17.063078 in #2
Epoch #3: 10240it [00:08, 1203.11it/s, env_step=30720, len=1000, loss=-0.036, loss/clip=-0.012, loss/ent=2.395, loss/vf=0.000, n/ep=0, n/st=256, rew=92.19]
Epoch #3: test_reward: 82.670980 Â± 64.692831, best_reward: 107.240348 Â± 17.063078 in #2
Epoch #4: 10240it [00:08, 1217.72it/s, env_step=40960, len=1000, loss=-0.026, loss/clip=-0.010, loss/ent=1.540, loss/vf=0.000, n/ep=0, n/st=256, rew=97.39]
Epoch #4: test_reward: -7.093884 Â± 34.274667, best_reward: 107.240348 Â± 17.063078 in #2
tracker requires_grad=True, has non-zero gradients=False
target requires_grad=True, has non-zero gradients=True
Training tracker policy with epoch 6
[34m[1mwandb[0m: [33mWARNING[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Loading agent under protect_vs_invade_adjusting_22
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #4: 10240it [00:09, 1084.77it/s, env_step=40960, len=0, loss=-0.030, loss/clip=-0.005, loss/ent=2.868, loss/vf=0.013, n/ep=0, n/st=256, rew=0.00] 
Epoch #4: test_reward: -496.670128 Â± 2.483617, best_reward: -496.670128 Â± 2.483617 in #4
Epoch #5: 10240it [00:07, 1329.72it/s, env_step=51200, len=1000, loss=-0.033, loss/clip=-0.007, loss/ent=2.592, loss/vf=0.000, n/ep=0, n/st=256, rew=-506.25]
Epoch #5: test_reward: -296.768164 Â± 176.192673, best_reward: -296.768164 Â± 176.192673 in #5
Epoch #6: 10240it [00:09, 1125.38it/s, env_step=61440, len=175, loss=-0.037, loss/clip=-0.009, loss/ent=2.808, loss/vf=0.002, n/ep=1, n/st=256, rew=-25.69]
