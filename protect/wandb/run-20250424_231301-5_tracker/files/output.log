Epoch #1: 10240it [00:12, 847.13it/s, env_step=10240, len=187, loss=0.122, loss/clip=-0.007, loss/ent=2.931, loss/vf=0.636, n/ep=4, n/st=256, rew=-47.78]                                                               
Epoch #1: test_reward: -78.590877 ± 6.332908, best_reward: -68.885436 ± 7.252058 in #0
Epoch #2: 10240it [00:10, 1002.63it/s, env_step=20480, len=114, loss=0.079, loss/clip=-0.001, loss/ent=3.122, loss/vf=0.445, n/ep=3, n/st=256, rew=-66.86]                                                              
Epoch #2: test_reward: -80.102424 ± 9.951810, best_reward: -68.885436 ± 7.252058 in #0
Epoch #3: 10240it [00:09, 1136.80it/s, env_step=30720, len=133, loss=0.099, loss/clip=-0.004, loss/ent=3.006, loss/vf=0.533, n/ep=1, n/st=256, rew=-74.93]                                                              
Epoch #3: test_reward: -76.066813 ± 7.033299, best_reward: -68.885436 ± 7.252058 in #0
Epoch #4: 10240it [00:10, 986.79it/s, env_step=40960, len=86, loss=0.170, loss/clip=-0.002, loss/ent=3.108, loss/vf=0.811, n/ep=0, n/st=256, rew=-61.46]                                                                
Epoch #4: test_reward: -71.238401 ± 8.701868, best_reward: -68.885436 ± 7.252058 in #0
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 4
