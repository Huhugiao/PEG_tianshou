Epoch #1: 10240it [00:09, 1122.01it/s, env_step=10240, len=115, loss=0.063, loss/clip=-0.004, loss/ent=3.153, loss/vf=0.394, n/ep=2, n/st=256, rew=-76.77]                                                              
Epoch #1: test_reward: -84.753661 Â± 15.592331, best_reward: -84.713705 Â± 17.432853 in #0
Epoch #2: 10240it [00:07, 1367.02it/s, env_step=20480, len=188, loss=0.016, loss/clip=-0.004, loss/ent=2.926, loss/vf=0.199, n/ep=2, n/st=256, rew=-89.97]                                                              
Epoch #2: test_reward: -84.335029 Â± 7.135289, best_reward: -84.335029 Â± 7.135289 in #2
Epoch #3: 10240it [00:06, 1477.85it/s, env_step=30720, len=132, loss=0.024, loss/clip=-0.008, loss/ent=2.559, loss/vf=0.228, n/ep=2, n/st=256, rew=-28.43]                                                              
Epoch #3: test_reward: -92.829984 Â± 19.615374, best_reward: -84.335029 Â± 7.135289 in #2
Epoch #4: 10240it [00:06, 1485.34it/s, env_step=40960, len=240, loss=0.022, loss/clip=-0.008, loss/ent=2.439, loss/vf=0.216, n/ep=2, n/st=256, rew=-105.68]                                                             
Epoch #4: test_reward: -84.233433 Â± 21.978995, best_reward: -84.233433 Â± 21.978995 in #4
Epoch #5: 10240it [00:06, 1473.58it/s, env_step=51200, len=150, loss=0.078, loss/clip=-0.005, loss/ent=2.627, loss/vf=0.438, n/ep=2, n/st=256, rew=-83.59]                                                              
Epoch #5: test_reward: -73.796925 Â± 55.378438, best_reward: -73.796925 Â± 55.378438 in #5
Epoch #6: 10240it [00:06, 1481.51it/s, env_step=61440, len=117, loss=0.042, loss/clip=-0.006, loss/ent=2.732, loss/vf=0.299, n/ep=1, n/st=256, rew=-70.86]                                                              
Epoch #6: test_reward: -73.419186 Â± 8.844095, best_reward: -73.419186 Â± 8.844095 in #6
Epoch #7: 10240it [00:06, 1482.31it/s, env_step=71680, len=122, loss=-0.030, loss/clip=-0.007, loss/ent=2.947, loss/vf=0.027, n/ep=1, n/st=256, rew=-76.60]                                                             
Epoch #7: test_reward: -66.074255 Â± 38.106835, best_reward: -66.074255 Â± 38.106835 in #7
Epoch #8: 10240it [00:06, 1479.44it/s, env_step=81920, len=87, loss=0.210, loss/clip=-0.008, loss/ent=2.678, loss/vf=0.981, n/ep=1, n/st=256, rew=-67.51]                                                               
Epoch #8: test_reward: -92.836479 Â± 13.345648, best_reward: -66.074255 Â± 38.106835 in #7
Epoch #9: 10240it [00:06, 1494.54it/s, env_step=92160, len=118, loss=0.050, loss/clip=-0.006, loss/ent=2.915, loss/vf=0.342, n/ep=1, n/st=256, rew=24.88]                                                               
Epoch #9: test_reward: -81.637381 Â± 14.742497, best_reward: -66.074255 Â± 38.106835 in #7
Epoch #10: 10240it [00:06, 1476.75it/s, env_step=102400, len=234, loss=0.130, loss/clip=-0.009, loss/ent=2.558, loss/vf=0.658, n/ep=5, n/st=256, rew=-104.44]                                                           
Epoch #10: test_reward: -66.009092 Â± 45.755145, best_reward: -66.009092 Â± 45.755145 in #10
Epoch #11: 10240it [00:06, 1470.95it/s, env_step=112640, len=107, loss=0.158, loss/clip=-0.005, loss/ent=2.781, loss/vf=0.766, n/ep=1, n/st=256, rew=-74.09]                                                            
Epoch #11: test_reward: -84.912683 Â± 19.555112, best_reward: -66.009092 Â± 45.755145 in #10
Epoch #12: 10240it [00:06, 1469.47it/s, env_step=122880, len=122, loss=0.198, loss/clip=-0.007, loss/ent=2.729, loss/vf=0.930, n/ep=0, n/st=256, rew=24.80]                                                             
Epoch #12: test_reward: -70.738696 Â± 50.043041, best_reward: -66.009092 Â± 45.755145 in #10
Epoch #13: 10240it [00:07, 1430.42it/s, env_step=133120, len=226, loss=0.163, loss/clip=-0.008, loss/ent=2.876, loss/vf=0.798, n/ep=2, n/st=256, rew=-97.24]                                                            
Epoch #13: test_reward: -32.026559 Â± 40.474658, best_reward: -32.026559 Â± 40.474658 in #13
Epoch #14: 10240it [00:07, 1433.04it/s, env_step=143360, len=141, loss=0.008, loss/clip=-0.009, loss/ent=2.628, loss/vf=0.171, n/ep=2, n/st=256, rew=-84.81]                                                            
Epoch #14: test_reward: -46.489993 Â± 43.470309, best_reward: -32.026559 Â± 40.474658 in #13
Epoch #15: 10240it [00:07, 1448.56it/s, env_step=153600, len=149, loss=0.138, loss/clip=-0.009, loss/ent=2.706, loss/vf=0.695, n/ep=3, n/st=256, rew=-12.54]                                                            
Epoch #15: test_reward: -29.395110 Â± 47.114503, best_reward: -29.395110 Â± 47.114503 in #15
Epoch #16: 10240it [00:07, 1449.07it/s, env_step=163840, len=409, loss=0.077, loss/clip=-0.006, loss/ent=2.928, loss/vf=0.450, n/ep=0, n/st=256, rew=-90.21]                                                            
Epoch #16: test_reward: -75.759987 Â± 23.398377, best_reward: -29.395110 Â± 47.114503 in #15
Epoch #17: 10240it [00:07, 1461.01it/s, env_step=174080, len=139, loss=0.031, loss/clip=-0.008, loss/ent=2.806, loss/vf=0.266, n/ep=0, n/st=256, rew=-82.57]                                                            
Epoch #17: test_reward: -20.339564 Â± 41.854075, best_reward: -20.339564 Â± 41.854075 in #17
Epoch #18: 10240it [00:07, 1445.42it/s, env_step=184320, len=137, loss=0.067, loss/clip=-0.010, loss/ent=2.702, loss/vf=0.419, n/ep=1, n/st=256, rew=24.84]                                                             
Epoch #18: test_reward: -12.910277 Â± 45.379406, best_reward: -12.910277 Â± 45.379406 in #18
Epoch #19: 10240it [00:06, 1483.99it/s, env_step=194560, len=155, loss=0.027, loss/clip=-0.009, loss/ent=2.799, loss/vf=0.258, n/ep=1, n/st=256, rew=17.57]                                                             
Epoch #19: test_reward: 10.210567 Â± 30.495514, best_reward: 10.210567 Â± 30.495514 in #19
Epoch #20: 10240it [00:06, 1481.03it/s, env_step=204800, len=59, loss=0.020, loss/clip=-0.011, loss/ent=2.526, loss/vf=0.227, n/ep=1, n/st=256, rew=45.05]                                                              
Epoch #20: test_reward: 28.079721 Â± 4.343855, best_reward: 28.079721 Â± 4.343855 in #20
Epoch #21: 10240it [00:06, 1491.03it/s, env_step=215040, len=98, loss=0.012, loss/clip=-0.012, loss/ent=2.473, loss/vf=0.194, n/ep=7, n/st=256, rew=-11.54]                                                             
Epoch #21: test_reward: -12.977664 Â± 47.394413, best_reward: 28.079721 Â± 4.343855 in #20
Epoch #22: 10240it [00:06, 1475.10it/s, env_step=225280, len=90, loss=0.029, loss/clip=-0.011, loss/ent=2.579, loss/vf=0.261, n/ep=3, n/st=256, rew=-31.23]                                                             
Epoch #22: test_reward: 40.883095 Â± 1.989192, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #23: 10240it [00:07, 1445.01it/s, env_step=235520, len=88, loss=0.026, loss/clip=-0.012, loss/ent=2.415, loss/vf=0.246, n/ep=3, n/st=256, rew=-32.22]                                                             
Epoch #23: test_reward: -52.830594 Â± 30.224236, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #24: 10240it [00:07, 1436.21it/s, env_step=245760, len=86, loss=0.003, loss/clip=-0.012, loss/ent=2.396, loss/vf=0.156, n/ep=2, n/st=256, rew=-16.85]                                                             
Epoch #24: test_reward: 33.371007 Â± 1.363110, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #25: 10240it [00:06, 1501.46it/s, env_step=256000, len=82, loss=0.063, loss/clip=-0.011, loss/ent=2.322, loss/vf=0.389, n/ep=5, n/st=256, rew=-5.00]                                                              
Epoch #25: test_reward: 25.614084 Â± 34.919596, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #26: 10240it [00:06, 1478.15it/s, env_step=266240, len=100, loss=0.016, loss/clip=-0.010, loss/ent=2.278, loss/vf=0.194, n/ep=2, n/st=256, rew=28.77]                                                             
Epoch #26: test_reward: 4.981139 Â± 44.629518, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #27: 10240it [00:06, 1498.94it/s, env_step=276480, len=52, loss=0.004, loss/clip=-0.014, loss/ent=2.154, loss/vf=0.161, n/ep=2, n/st=256, rew=43.12]                                                              
Epoch #27: test_reward: 37.922554 Â± 2.608789, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #28: 10240it [00:06, 1464.97it/s, env_step=286720, len=92, loss=0.029, loss/clip=-0.012, loss/ent=2.312, loss/vf=0.259, n/ep=0, n/st=256, rew=-16.37]                                                             
Epoch #28: test_reward: -7.105826 Â± 49.004380, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #29: 10240it [00:06, 1472.89it/s, env_step=296960, len=74, loss=0.015, loss/clip=-0.010, loss/ent=2.377, loss/vf=0.193, n/ep=2, n/st=256, rew=37.12]                                                              
Epoch #29: test_reward: 22.834180 Â± 34.099283, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #30: 10240it [00:06, 1493.78it/s, env_step=307200, len=90, loss=0.010, loss/clip=-0.011, loss/ent=2.415, loss/vf=0.182, n/ep=1, n/st=256, rew=-68.13]                                                             
Epoch #30: test_reward: -20.047592 Â± 49.671677, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #31: 10240it [00:06, 1490.83it/s, env_step=317440, len=84, loss=-0.018, loss/clip=-0.015, loss/ent=2.286, loss/vf=0.082, n/ep=3, n/st=256, rew=35.98]                                                             
Epoch #31: test_reward: -12.226134 Â± 47.578602, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #32: 10240it [00:06, 1489.56it/s, env_step=327680, len=126, loss=0.018, loss/clip=-0.013, loss/ent=2.260, loss/vf=0.215, n/ep=3, n/st=256, rew=-8.94]                                                             
Epoch #32: test_reward: 40.126631 Â± 2.147372, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #33: 10240it [00:06, 1508.61it/s, env_step=337920, len=81, loss=-0.018, loss/clip=-0.015, loss/ent=2.369, loss/vf=0.084, n/ep=2, n/st=256, rew=38.17]                                                             
Epoch #33: test_reward: 39.926454 Â± 2.588154, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #34: 10240it [00:06, 1481.52it/s, env_step=348160, len=98, loss=0.005, loss/clip=-0.009, loss/ent=2.399, loss/vf=0.152, n/ep=3, n/st=256, rew=-3.36]                                                              
Epoch #34: test_reward: 25.157930 Â± 33.646801, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #35: 10240it [00:07, 1380.62it/s, env_step=358400, len=110, loss=0.035, loss/clip=-0.012, loss/ent=2.426, loss/vf=0.285, n/ep=5, n/st=256, rew=-31.43]                                                            
Epoch #35: test_reward: -16.460308 Â± 52.343620, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #36: 10240it [00:06, 1481.17it/s, env_step=368640, len=97, loss=0.011, loss/clip=-0.016, loss/ent=2.456, loss/vf=0.207, n/ep=2, n/st=256, rew=-20.94]                                                             
Epoch #36: test_reward: 17.776350 Â± 32.917287, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #37: 10240it [00:07, 1455.68it/s, env_step=378880, len=94, loss=-0.030, loss/clip=-0.015, loss/ent=2.380, loss/vf=0.036, n/ep=0, n/st=256, rew=34.31]                                                             
Epoch #37: test_reward: -5.302453 Â± 48.010325, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #38: 10240it [00:06, 1476.26it/s, env_step=389120, len=112, loss=-0.001, loss/clip=-0.013, loss/ent=2.367, loss/vf=0.144, n/ep=3, n/st=256, rew=-8.48]                                                            
Epoch #38: test_reward: -25.393263 Â± 47.879454, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #39: 10240it [00:06, 1477.66it/s, env_step=399360, len=71, loss=0.018, loss/clip=-0.012, loss/ent=2.368, loss/vf=0.218, n/ep=5, n/st=256, rew=18.98]                                                              
Epoch #39: test_reward: 39.701544 Â± 3.344852, best_reward: 40.883095 Â± 1.989192 in #22
Epoch #40: 10240it [00:06, 1490.73it/s, env_step=409600, len=97, loss=-0.035, loss/clip=-0.019, loss/ent=2.172, loss/vf=0.022, n/ep=6, n/st=256, rew=34.50]                                                             
Epoch #40: test_reward: 24.476359 Â± 2.169769, best_reward: 40.883095 Â± 1.989192 in #22
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 80
Loading agent under protect_vs_invade_adjusting_20
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #40: 10240it [00:09, 1136.65it/s, env_step=409600, len=311, loss=-0.031, loss/clip=-0.006, loss/ent=2.883, loss/vf=0.015, n/ep=0, n/st=256, rew=-6.38]                                                            
Epoch #40: test_reward: 19.511256 Â± 64.903793, best_reward: 106.753519 Â± 35.518125 in #39
Epoch #41: 10240it [00:07, 1373.49it/s, env_step=419840, len=1000, loss=-0.039, loss/clip=-0.010, loss/ent=2.998, loss/vf=0.000, n/ep=0, n/st=256, rew=140.18]                                                          
Epoch #41: test_reward: -43.874795 Â± 3.293983, best_reward: 106.753519 Â± 35.518125 in #39
Epoch #42: 10240it [00:07, 1321.31it/s, env_step=430080, len=386, loss=-0.018, loss/clip=-0.007, loss/ent=3.006, loss/vf=0.076, n/ep=0, n/st=256, rew=25.55]                                                            
Epoch #42: test_reward: 58.736924 Â± 12.425860, best_reward: 106.753519 Â± 35.518125 in #39
Epoch #43: 10240it [00:07, 1345.53it/s, env_step=440320, len=734, loss=-0.038, loss/clip=-0.008, loss/ent=2.995, loss/vf=0.000, n/ep=0, n/st=256, rew=152.87]                                                           
Epoch #43: test_reward: 77.957982 Â± 77.562978, best_reward: 106.753519 Â± 35.518125 in #39
Epoch #44: 10240it [00:07, 1323.71it/s, env_step=450560, len=804, loss=-0.036, loss/clip=-0.008, loss/ent=2.811, loss/vf=0.000, n/ep=0, n/st=256, rew=96.83]                                                            
Epoch #44: test_reward: 141.941458 Â± 71.177789, best_reward: 141.941458 Â± 71.177789 in #44
Epoch #45: 10240it [00:07, 1292.71it/s, env_step=460800, len=1000, loss=-0.031, loss/clip=-0.012, loss/ent=1.875, loss/vf=0.000, n/ep=0, n/st=256, rew=203.00]                                                          
Epoch #45: test_reward: 177.142376 Â± 7.577019, best_reward: 177.142376 Â± 7.577019 in #45
Epoch #46: 10240it [00:07, 1318.68it/s, env_step=471040, len=36, loss=-0.037, loss/clip=-0.013, loss/ent=2.747, loss/vf=0.017, n/ep=0, n/st=256, rew=-45.45]                                                            
Epoch #46: test_reward: 196.413304 Â± 54.141478, best_reward: 196.413304 Â± 54.141478 in #46
Epoch #47: 10240it [00:07, 1319.39it/s, env_step=481280, len=1000, loss=-0.033, loss/clip=-0.006, loss/ent=2.654, loss/vf=0.000, n/ep=0, n/st=256, rew=220.65]                                                          
Epoch #47: test_reward: 207.252330 Â± 1.563701, best_reward: 207.252330 Â± 1.563701 in #47
Epoch #48: 10240it [00:07, 1318.08it/s, env_step=491520, len=892, loss=-0.019, loss/clip=-0.007, loss/ent=2.708, loss/vf=0.062, n/ep=0, n/st=256, rew=183.63]                                                           
Epoch #48: test_reward: 202.389587 Â± 1.641641, best_reward: 207.252330 Â± 1.563701 in #47
Epoch #49: 10240it [00:07, 1349.67it/s, env_step=501760, len=1000, loss=-0.034, loss/clip=-0.008, loss/ent=2.660, loss/vf=0.000, n/ep=0, n/st=256, rew=231.03]                                                          
Epoch #49: test_reward: 92.282135 Â± 137.003422, best_reward: 207.252330 Â± 1.563701 in #47
Epoch #50: 10240it [00:07, 1310.43it/s, env_step=512000, len=529, loss=-0.031, loss/clip=-0.007, loss/ent=2.663, loss/vf=0.012, n/ep=0, n/st=256, rew=97.17]                                                            
Epoch #50: test_reward: 231.675566 Â± 0.800714, best_reward: 231.675566 Â± 0.800714 in #50
Epoch #51: 10240it [00:07, 1317.29it/s, env_step=522240, len=1000, loss=-0.037, loss/clip=-0.009, loss/ent=2.757, loss/vf=0.000, n/ep=0, n/st=256, rew=231.60]                                                          
Epoch #51: test_reward: 240.953574 Â± 0.739417, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #52: 10240it [00:07, 1337.27it/s, env_step=532480, len=1000, loss=-0.036, loss/clip=-0.010, loss/ent=2.604, loss/vf=0.000, n/ep=0, n/st=256, rew=242.12]                                                          
Epoch #52: test_reward: -32.749143 Â± 34.123673, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #53: 10240it [00:07, 1303.43it/s, env_step=542720, len=1000, loss=-0.038, loss/clip=-0.013, loss/ent=2.537, loss/vf=0.000, n/ep=0, n/st=256, rew=223.77]                                                          
Epoch #53: test_reward: -28.961844 Â± 34.595401, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #54: 10240it [00:07, 1338.07it/s, env_step=552960, len=1000, loss=-0.034, loss/clip=-0.010, loss/ent=2.434, loss/vf=0.000, n/ep=0, n/st=256, rew=206.46]                                                          
Epoch #54: test_reward: 89.924337 Â± 97.860443, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #55: 10240it [00:07, 1319.86it/s, env_step=563200, len=1000, loss=-0.032, loss/clip=-0.010, loss/ent=2.210, loss/vf=0.000, n/ep=0, n/st=256, rew=213.77]                                                          
Epoch #55: test_reward: 208.295058 Â± 0.846971, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #56: 10240it [00:07, 1302.66it/s, env_step=573440, len=1000, loss=-0.032, loss/clip=-0.009, loss/ent=2.250, loss/vf=0.000, n/ep=0, n/st=256, rew=219.74]                                                          
Epoch #56: test_reward: 222.777052 Â± 0.562024, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #57: 10240it [00:07, 1326.96it/s, env_step=583680, len=1000, loss=-0.025, loss/clip=-0.007, loss/ent=1.739, loss/vf=0.000, n/ep=0, n/st=256, rew=230.08]                                                          
Epoch #57: test_reward: 237.850232 Â± 0.762472, best_reward: 240.953574 Â± 0.739417 in #51
Epoch #58: 10240it [00:07, 1331.53it/s, env_step=593920, len=1000, loss=-0.029, loss/clip=-0.009, loss/ent=1.980, loss/vf=0.000, n/ep=0, n/st=256, rew=231.27]                                                          
Epoch #58: test_reward: 248.023312 Â± 1.248000, best_reward: 248.023312 Â± 1.248000 in #58
Epoch #59: 10240it [00:07, 1294.20it/s, env_step=604160, len=1000, loss=-0.028, loss/clip=-0.007, loss/ent=2.197, loss/vf=0.000, n/ep=1, n/st=256, rew=243.10]                                                          
Epoch #59: test_reward: 216.399791 Â± 96.935387, best_reward: 248.023312 Â± 1.248000 in #58
Epoch #60: 10240it [00:07, 1356.36it/s, env_step=614400, len=1000, loss=-0.038, loss/clip=-0.013, loss/ent=2.484, loss/vf=0.000, n/ep=1, n/st=256, rew=256.09]                                                          
Epoch #60: test_reward: -14.385130 Â± 46.251500, best_reward: 248.023312 Â± 1.248000 in #58
Epoch #61: 10240it [00:08, 1233.47it/s, env_step=624640, len=1000, loss=-0.038, loss/clip=-0.011, loss/ent=2.627, loss/vf=0.000, n/ep=0, n/st=256, rew=261.06]                                                          
Epoch #61: test_reward: 266.380719 Â± 2.254796, best_reward: 266.380719 Â± 2.254796 in #61
Epoch #62: 10240it [00:07, 1282.31it/s, env_step=634880, len=1000, loss=-0.032, loss/clip=-0.008, loss/ent=2.335, loss/vf=0.000, n/ep=0, n/st=256, rew=267.75]                                                          
Epoch #62: test_reward: 281.129096 Â± 4.137327, best_reward: 281.129096 Â± 4.137327 in #62
Epoch #63: 10240it [00:07, 1332.43it/s, env_step=645120, len=1000, loss=-0.031, loss/clip=-0.009, loss/ent=2.254, loss/vf=0.000, n/ep=1, n/st=256, rew=276.24]                                                          
Epoch #63: test_reward: 286.798101 Â± 2.936374, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #64: 10240it [00:07, 1299.14it/s, env_step=655360, len=1000, loss=-0.033, loss/clip=-0.010, loss/ent=2.244, loss/vf=0.000, n/ep=0, n/st=256, rew=277.07]                                                          
Epoch #64: test_reward: -29.570429 Â± 33.992186, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #65: 10240it [00:07, 1336.98it/s, env_step=665600, len=39, loss=-0.029, loss/clip=-0.008, loss/ent=2.107, loss/vf=0.000, n/ep=0, n/st=256, rew=56.86]                                                             
Epoch #65: test_reward: 274.458842 Â± 1.078557, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #66: 10240it [00:07, 1343.92it/s, env_step=675840, len=39, loss=-0.025, loss/clip=-0.009, loss/ent=2.100, loss/vf=0.023, n/ep=0, n/st=256, rew=57.35]                                                             
Epoch #66: test_reward: -46.578729 Â± 0.276984, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #67: 10240it [00:07, 1321.39it/s, env_step=686080, len=33, loss=0.336, loss/clip=-0.012, loss/ent=1.703, loss/vf=1.461, n/ep=0, n/st=256, rew=5.67]                                                               
Epoch #67: test_reward: 20.954430 Â± 116.316923, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #68: 10240it [00:07, 1343.51it/s, env_step=696320, len=1000, loss=0.045, loss/clip=-0.009, loss/ent=2.033, loss/vf=0.299, n/ep=0, n/st=256, rew=270.84]                                                           
Epoch #68: test_reward: 82.399088 Â± 147.860565, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #69: 10240it [00:07, 1337.48it/s, env_step=706560, len=1000, loss=-0.003, loss/clip=-0.008, loss/ent=2.240, loss/vf=0.111, n/ep=1, n/st=256, rew=269.60]                                                          
Epoch #69: test_reward: 86.372921 Â± 143.990403, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #70: 10240it [00:07, 1343.34it/s, env_step=716800, len=36, loss=0.163, loss/clip=-0.008, loss/ent=1.954, loss/vf=0.762, n/ep=1, n/st=256, rew=-43.45]                                                             
Epoch #70: test_reward: 112.651041 Â± 156.393323, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #71: 10240it [00:07, 1349.30it/s, env_step=727040, len=1000, loss=-0.030, loss/clip=-0.010, loss/ent=2.272, loss/vf=0.008, n/ep=0, n/st=256, rew=269.51]                                                          
Epoch #71: test_reward: 112.406567 Â± 152.596214, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #72: 10240it [00:07, 1322.32it/s, env_step=737280, len=48, loss=0.216, loss/clip=-0.007, loss/ent=1.699, loss/vf=0.962, n/ep=2, n/st=256, rew=-41.68]                                                             
Epoch #72: test_reward: 72.470068 Â± 152.195665, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #73: 10240it [00:07, 1377.05it/s, env_step=747520, len=65, loss=0.097, loss/clip=-0.012, loss/ent=2.041, loss/vf=0.519, n/ep=0, n/st=256, rew=-37.60]                                                             
Epoch #73: test_reward: 155.147933 Â± 151.648400, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #74: 10240it [00:07, 1331.11it/s, env_step=757760, len=27, loss=0.091, loss/clip=-0.005, loss/ent=1.992, loss/vf=0.464, n/ep=0, n/st=256, rew=-45.85]                                                             
Epoch #74: test_reward: -3.847505 Â± 48.970327, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #75: 10240it [00:07, 1311.01it/s, env_step=768000, len=1000, loss=-0.032, loss/clip=-0.009, loss/ent=2.282, loss/vf=0.000, n/ep=0, n/st=256, rew=265.94]                                                          
Epoch #75: test_reward: 225.589388 Â± 101.343577, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #76: 10240it [00:07, 1308.64it/s, env_step=778240, len=1000, loss=-0.032, loss/clip=-0.011, loss/ent=2.149, loss/vf=0.001, n/ep=0, n/st=256, rew=264.31]                                                          
Epoch #76: test_reward: 228.501615 Â± 102.592069, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #77: 10240it [00:07, 1324.96it/s, env_step=788480, len=1000, loss=-0.034, loss/clip=-0.015, loss/ent=2.053, loss/vf=0.007, n/ep=1, n/st=256, rew=267.75]                                                          
Epoch #77: test_reward: 265.048141 Â± 0.463958, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #78: 10240it [00:07, 1335.13it/s, env_step=798720, len=1000, loss=-0.004, loss/clip=-0.014, loss/ent=1.755, loss/vf=0.107, n/ep=0, n/st=256, rew=266.02]                                                          
Epoch #78: test_reward: 225.439581 Â± 100.410018, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #79: 10240it [00:12, 851.13it/s, env_step=808960, len=1000, loss=-0.020, loss/clip=-0.008, loss/ent=1.235, loss/vf=0.001, n/ep=1, n/st=256, rew=262.69]                                                           
Epoch #79: test_reward: 148.486834 Â± 150.303162, best_reward: 286.798101 Â± 2.936374 in #63
Epoch #80: 10240it [00:10, 1021.30it/s, env_step=819200, len=334, loss=0.138, loss/clip=-0.005, loss/ent=1.118, loss/vf=0.618, n/ep=2, n/st=256, rew=33.75]                                                             
Epoch #80: test_reward: 6.262156 Â± 81.655660, best_reward: 286.798101 Â± 2.936374 in #63
tracker requires_grad=True, has non-zero gradients=False
target requires_grad=True, has non-zero gradients=True
Training tracker policy with epoch 120
Loading agent under protect_vs_invade_adjusting_20
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #80: 10240it [00:10, 1002.34it/s, env_step=819200, len=0, loss=-0.040, loss/clip=-0.016, loss/ent=2.610, loss/vf=0.009, n/ep=0, n/st=256, rew=0.00]                                                               
Epoch #80: test_reward: -242.281740 Â± 27.268807, best_reward: -174.711135 Â± 41.246128 in #79
Epoch #81: 10240it [00:07, 1315.62it/s, env_step=829440, len=1000, loss=-0.038, loss/clip=-0.011, loss/ent=2.665, loss/vf=0.001, n/ep=0, n/st=256, rew=-228.59]                                                         
Epoch #81: test_reward: -258.705805 Â± 54.557848, best_reward: -174.711135 Â± 41.246128 in #79
Epoch #82: 10240it [00:08, 1219.90it/s, env_step=839680, len=1000, loss=-0.036, loss/clip=-0.011, loss/ent=2.579, loss/vf=0.001, n/ep=0, n/st=256, rew=-156.97]                                                         
Epoch #82: test_reward: -167.965370 Â± 95.693291, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #83: 10240it [00:09, 1056.53it/s, env_step=849920, len=1000, loss=-0.033, loss/clip=-0.011, loss/ent=2.277, loss/vf=0.000, n/ep=0, n/st=256, rew=-156.97]                                                         
Epoch #83: test_reward: -274.675794 Â± 38.014514, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #84: 10240it [00:09, 1035.82it/s, env_step=860160, len=42, loss=-0.035, loss/clip=-0.011, loss/ent=2.382, loss/vf=0.000, n/ep=0, n/st=256, rew=46.43]                                                             
Epoch #84: test_reward: -273.342341 Â± 25.631068, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #85: 10240it [00:09, 1097.41it/s, env_step=870400, len=1000, loss=-0.028, loss/clip=-0.018, loss/ent=1.801, loss/vf=0.033, n/ep=0, n/st=256, rew=-299.79]                                                         
Epoch #85: test_reward: -274.605979 Â± 9.598152, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #86: 10240it [00:09, 1117.34it/s, env_step=880640, len=1000, loss=-0.028, loss/clip=-0.012, loss/ent=1.578, loss/vf=0.000, n/ep=0, n/st=256, rew=-299.79]                                                         
Epoch #86: test_reward: -285.953073 Â± 1.626021, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #87: 10240it [00:09, 1101.26it/s, env_step=890880, len=1000, loss=-0.020, loss/clip=-0.008, loss/ent=1.163, loss/vf=0.000, n/ep=0, n/st=256, rew=-289.31]                                                         
Epoch #87: test_reward: -294.530535 Â± 0.571827, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #88: 10240it [00:09, 1113.88it/s, env_step=901120, len=1000, loss=-0.022, loss/clip=-0.012, loss/ent=0.990, loss/vf=0.000, n/ep=0, n/st=256, rew=-289.43]                                                         
Epoch #88: test_reward: -287.621816 Â± 0.868191, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #89: 10240it [00:08, 1276.14it/s, env_step=911360, len=30, loss=-0.011, loss/clip=-0.005, loss/ent=0.628, loss/vf=0.000, n/ep=0, n/st=256, rew=44.76]                                                             
Epoch #89: test_reward: -278.874753 Â± 1.214037, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #90: 10240it [00:10, 1009.10it/s, env_step=921600, len=1000, loss=-0.021, loss/clip=-0.015, loss/ent=0.590, loss/vf=0.000, n/ep=0, n/st=256, rew=-290.61]                                                         
Epoch #90: test_reward: -278.419785 Â± 1.882115, best_reward: -167.965370 Â± 95.693291 in #82
Epoch #91: 10240it [00:10, 998.58it/s, env_step=931840, len=1000, loss=-0.015, loss/clip=-0.011, loss/ent=0.413, loss/vf=0.000, n/ep=0, n/st=256, rew=-290.61]                                                          
Epoch #91: test_reward: -117.072058 Â± 163.698979, best_reward: -117.072058 Â± 163.698979 in #91
Epoch #92: 10240it [00:09, 1107.27it/s, env_step=942080, len=1000, loss=-0.028, loss/clip=-0.015, loss/ent=1.291, loss/vf=0.000, n/ep=0, n/st=256, rew=-22.57]                                                          
Epoch #92: test_reward: -205.611458 Â± 54.275619, best_reward: -117.072058 Â± 163.698979 in #91
Epoch #93: 10240it [00:08, 1153.56it/s, env_step=952320, len=1000, loss=-0.035, loss/clip=-0.023, loss/ent=1.190, loss/vf=0.000, n/ep=0, n/st=256, rew=-275.98]                                                         
Epoch #93: test_reward: -140.268951 Â± 1.791037, best_reward: -117.072058 Â± 163.698979 in #91
Epoch #94: 10240it [00:09, 1114.19it/s, env_step=962560, len=1000, loss=-0.028, loss/clip=-0.012, loss/ent=1.643, loss/vf=0.000, n/ep=0, n/st=256, rew=-275.98]                                                         
Epoch #94: test_reward: -31.553298 Â± 50.731473, best_reward: -31.553298 Â± 50.731473 in #94
Epoch #95: 10240it [00:10, 996.08it/s, env_step=972800, len=1000, loss=-0.027, loss/clip=-0.019, loss/ent=0.888, loss/vf=0.000, n/ep=0, n/st=256, rew=-98.26]                                                           
Epoch #95: test_reward: -37.730682 Â± 36.211113, best_reward: -31.553298 Â± 50.731473 in #94
Epoch #96: 10240it [00:08, 1207.69it/s, env_step=983040, len=90, loss=-0.025, loss/clip=-0.019, loss/ent=1.145, loss/vf=0.024, n/ep=0, n/st=256, rew=40.50]                                                             
Epoch #96: test_reward: -73.089455 Â± 14.812937, best_reward: -31.553298 Â± 50.731473 in #94
Epoch #97: 10240it [00:08, 1161.22it/s, env_step=993280, len=763, loss=-0.027, loss/clip=-0.018, loss/ent=1.462, loss/vf=0.025, n/ep=2, n/st=256, rew=15.88]                                                            
Epoch #97: test_reward: 47.867101 Â± 3.521398, best_reward: 47.867101 Â± 3.521398 in #97
Epoch #98: 10240it [00:09, 1124.49it/s, env_step=1003520, len=304, loss=-0.037, loss/clip=-0.023, loss/ent=1.562, loss/vf=0.007, n/ep=1, n/st=256, rew=37.90]                                                           
Epoch #98: test_reward: 5.780242 Â± 5.334688, best_reward: 47.867101 Â± 3.521398 in #97
Epoch #99: 10240it [00:08, 1193.80it/s, env_step=1013760, len=1000, loss=-0.050, loss/clip=-0.033, loss/ent=1.761, loss/vf=0.000, n/ep=0, n/st=256, rew=37.78]                                                          
Epoch #99: test_reward: 48.665614 Â± 10.348151, best_reward: 48.665614 Â± 10.348151 in #99
Epoch #100: 10240it [00:08, 1151.72it/s, env_step=1024000, len=1000, loss=-0.034, loss/clip=-0.017, loss/ent=1.734, loss/vf=0.000, n/ep=0, n/st=256, rew=24.75]                                                         
