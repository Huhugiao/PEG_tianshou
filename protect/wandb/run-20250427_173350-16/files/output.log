Epoch #1: 10240it [00:09, 1052.91it/s, env_step=10240, len=101, loss=-0.035, loss/clip=-0.009, loss/ent=2.860, loss/vf=0.007, n/ep=4, n/st=256, rew=-108.59]
Epoch #1: test_reward: -142.419682 Â± 10.972604, best_reward: -103.250665 Â± 36.088257 in #0
Epoch #2: 10240it [00:07, 1368.99it/s, env_step=20480, len=173, loss=-0.031, loss/clip=-0.005, loss/ent=2.652, loss/vf=0.002, n/ep=1, n/st=256, rew=-141.76]
Epoch #2: test_reward: -131.519958 Â± 33.569129, best_reward: -103.250665 Â± 36.088257 in #0
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 4
Loading agent under protect_vs_invade_adjusting_24
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #2: 10240it [00:10, 970.13it/s, env_step=20480, len=553, loss=-0.031, loss/clip=-0.006, loss/ent=2.829, loss/vf=0.012, n/ep=0, n/st=256, rew=100.53] 
Epoch #2: test_reward: 185.028609 Â± 14.977316, best_reward: 185.028609 Â± 14.977316 in #2
Epoch #3: 10240it [00:08, 1166.42it/s, env_step=30720, len=1000, loss=-0.037, loss/clip=-0.007, loss/ent=2.961, loss/vf=0.000, n/ep=1, n/st=256, rew=167.88]
Epoch #3: test_reward: 88.895989 Â± 6.788062, best_reward: 185.028609 Â± 14.977316 in #2
Epoch #4: 10240it [00:08, 1187.02it/s, env_step=40960, len=1000, loss=-0.038, loss/clip=-0.008, loss/ent=2.963, loss/vf=0.001, n/ep=0, n/st=256, rew=129.71]
Epoch #4: test_reward: 139.007412 Â± 31.185334, best_reward: 185.028609 Â± 14.977316 in #2
tracker requires_grad=True, has non-zero gradients=False
target requires_grad=True, has non-zero gradients=True
Training tracker policy with epoch 6
Loading agent under protect_vs_invade_adjusting_24
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #4: 10240it [00:09, 1032.21it/s, env_step=40960, len=0, loss=-0.030, loss/clip=-0.005, loss/ent=2.729, loss/vf=0.007, n/ep=0, n/st=256, rew=0.00]
Epoch #4: test_reward: -532.362437 Â± 6.196276, best_reward: -532.362437 Â± 6.196276 in #4
Epoch #5: 10240it [00:08, 1192.88it/s, env_step=51200, len=1000, loss=-0.021, loss/clip=-0.002, loss/ent=1.934, loss/vf=0.000, n/ep=0, n/st=256, rew=-521.42]
Epoch #5: test_reward: -511.348202 Â± 2.367154, best_reward: -511.348202 Â± 2.367154 in #5
Epoch #6: 10240it [00:09, 1082.57it/s, env_step=61440, len=1000, loss=-0.018, loss/clip=-0.004, loss/ent=1.422, loss/vf=0.000, n/ep=0, n/st=256, rew=-519.54]
Epoch #6: test_reward: -544.209235 Â± 2.767294, best_reward: -511.348202 Â± 2.367154 in #5
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 8
Loading agent under protect_vs_invade_adjusting_24
[34m[1mwandb[0m: [33mWARNING[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #4: 10240it [00:10, 972.71it/s, env_step=40960, len=273, loss=-0.035, loss/clip=-0.008, loss/ent=2.844, loss/vf=0.004, n/ep=0, n/st=256, rew=-29.70] 
Epoch #4: test_reward: 207.597534 Â± 31.638870, best_reward: 207.597534 Â± 31.638870 in #4
Epoch #5: 10240it [00:08, 1210.98it/s, env_step=51200, len=1000, loss=-0.039, loss/clip=-0.012, loss/ent=2.639, loss/vf=0.000, n/ep=0, n/st=256, rew=279.42]
Epoch #5: test_reward: 52.743642 Â± 29.592417, best_reward: 207.597534 Â± 31.638870 in #4
Epoch #6: 10240it [00:10, 983.71it/s, env_step=61440, len=118, loss=-0.037, loss/clip=-0.009, loss/ent=2.744, loss/vf=0.001, n/ep=0, n/st=256, rew=64.90] 
Epoch #6: test_reward: 279.545214 Â± 7.349281, best_reward: 279.545214 Â± 7.349281 in #6
Epoch #7: 10240it [00:08, 1193.84it/s, env_step=71680, len=796, loss=-0.044, loss/clip=-0.018, loss/ent=2.687, loss/vf=0.001, n/ep=0, n/st=256, rew=223.84]
Epoch #7: test_reward: 97.784990 Â± 9.223354, best_reward: 279.545214 Â± 7.349281 in #6
Epoch #8: 10240it [00:09, 1130.43it/s, env_step=81920, len=235, loss=-0.041, loss/clip=-0.014, loss/ent=2.764, loss/vf=0.000, n/ep=1, n/st=256, rew=109.01]
Epoch #8: test_reward: 85.611899 Â± 26.856836, best_reward: 279.545214 Â± 7.349281 in #6
tracker requires_grad=True, has non-zero gradients=False
target requires_grad=True, has non-zero gradients=True
Training tracker policy with epoch 10
Loading agent under protect_vs_invade_adjusting_24
[34m[1mwandb[0m:   1 of 1 files downloaded.
Epoch #4: 10240it [00:10, 968.57it/s, env_step=40960, len=0, loss=-0.027, loss/clip=-0.003, loss/ent=2.605, loss/vf=0.008, n/ep=0, n/st=256, rew=0.00] 
Epoch #4: test_reward: -488.011938 Â± 5.597997, best_reward: -488.011938 Â± 5.597997 in #4
Epoch #5: 10240it [00:09, 1050.35it/s, env_step=51200, len=125, loss=-0.029, loss/clip=-0.007, loss/ent=2.223, loss/vf=0.000, n/ep=0, n/st=256, rew=-132.81]
Epoch #5: test_reward: -368.667482 Â± 211.207710, best_reward: -368.667482 Â± 211.207710 in #5
Epoch #6: 10240it [00:08, 1195.47it/s, env_step=61440, len=1000, loss=-0.033, loss/clip=-0.004, loss/ent=2.939, loss/vf=0.001, n/ep=0, n/st=256, rew=-544.78]
Epoch #6: test_reward: -503.151219 Â± 4.850906, best_reward: -368.667482 Â± 211.207710 in #5
Epoch #7: 10240it [00:09, 1048.19it/s, env_step=71680, len=1000, loss=-0.025, loss/clip=-0.005, loss/ent=2.051, loss/vf=0.000, n/ep=0, n/st=256, rew=-544.78]                                                    
Epoch #7: test_reward: -495.111865 Â± 4.213142, best_reward: -368.667482 Â± 211.207710 in #5
Epoch #8: 10240it [00:09, 1062.91it/s, env_step=81920, len=1000, loss=-0.028, loss/clip=-0.006, loss/ent=2.200, loss/vf=0.000, n/ep=0, n/st=256, rew=-495.23]                                                    
Epoch #8: test_reward: 30.320176 Â± 1.353469, best_reward: 30.320176 Â± 1.353469 in #8
Epoch #9:  20%|#######3                            | 2048/10000 [00:01<00:06, 1266.55it/s, env_step=83968, len=1000, loss=-0.025, loss/clip=-0.005, loss/ent=1.948, loss/vf=0.000, n/ep=0, n/st=256, rew=-495.23]
