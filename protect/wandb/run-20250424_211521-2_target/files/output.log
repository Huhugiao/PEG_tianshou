Loading agent under protect_vs_invade
Successfully restored tracker policy and optimizer.
Epoch #1: 10240it [00:09, 1126.58it/s, env_step=10240, len=680, loss=-0.030, loss/clip=-0.001, loss/ent=3.177, loss/vf=0.010, n/ep=0, n/st=256, rew=37.62]              
Epoch #1: test_reward: 108.327800 ± 12.983435, best_reward: 108.327800 ± 12.983435 in #1
Epoch #2: 10240it [00:07, 1337.99it/s, env_step=20480, len=530, loss=1.034, loss/clip=-0.005, loss/ent=3.145, loss/vf=4.282, n/ep=2, n/st=256, rew=126.52]              
Epoch #2: test_reward: 108.057586 ± 33.647482, best_reward: 108.327800 ± 12.983435 in #1
Epoch #3: 10240it [00:07, 1352.76it/s, env_step=30720, len=125, loss=1.569, loss/clip=-0.003, loss/ent=3.092, loss/vf=6.411, n/ep=2, n/st=256, rew=18.44]               
Epoch #3: test_reward: 60.625371 ± 32.222512, best_reward: 108.327800 ± 12.983435 in #1
Epoch #4: 10240it [00:07, 1397.82it/s, env_step=40960, len=84, loss=0.407, loss/clip=-0.006, loss/ent=2.863, loss/vf=1.768, n/ep=4, n/st=256, rew=61.69]                
Epoch #4: test_reward: 59.505908 ± 1.651670, best_reward: 108.327800 ± 12.983435 in #1
Epoch #5: 10240it [00:08, 1270.49it/s, env_step=51200, len=57, loss=0.329, loss/clip=-0.010, loss/ent=2.424, loss/vf=1.453, n/ep=4, n/st=256, rew=32.99]                
Epoch #5: test_reward: 45.559237 ± 32.898162, best_reward: 108.327800 ± 12.983435 in #1
Epoch #6: 10240it [00:07, 1428.28it/s, env_step=61440, len=52, loss=0.252, loss/clip=-0.008, loss/ent=2.006, loss/vf=1.120, n/ep=3, n/st=256, rew=23.83]                
Epoch #6: test_reward: 44.502257 ± 33.231942, best_reward: 108.327800 ± 12.983435 in #1
Epoch #7: 10240it [00:07, 1444.75it/s, env_step=71680, len=51, loss=0.571, loss/clip=-0.006, loss/ent=1.456, loss/vf=2.366, n/ep=5, n/st=256, rew=-23.71]               
Epoch #7: test_reward: 31.956800 ± 43.114291, best_reward: 108.327800 ± 12.983435 in #1
Epoch #8: 10240it [00:07, 1396.22it/s, env_step=81920, len=54, loss=0.385, loss/clip=-0.010, loss/ent=1.925, loss/vf=1.657, n/ep=6, n/st=256, rew=-9.70]                
Epoch #8: test_reward: 56.736332 ± 0.211830, best_reward: 108.327800 ± 12.983435 in #1
Epoch #9: 10240it [00:07, 1427.50it/s, env_step=92160, len=76, loss=0.171, loss/clip=-0.019, loss/ent=2.320, loss/vf=0.852, n/ep=2, n/st=256, rew=59.80]                
Epoch #9: test_reward: 9.557752 ± 49.519167, best_reward: 108.327800 ± 12.983435 in #1
Epoch #10: 10240it [00:06, 1519.49it/s, env_step=102400, len=48, loss=0.222, loss/clip=-0.014, loss/ent=2.064, loss/vf=1.026, n/ep=9, n/st=256, rew=12.13]              
Epoch #10: test_reward: 44.906788 ± 33.163276, best_reward: 108.327800 ± 12.983435 in #1
tracker requires_grad=True, has non-zero gradients=False
target requires_grad=True, has non-zero gradients=True
Training tracker policy with epoch 15
