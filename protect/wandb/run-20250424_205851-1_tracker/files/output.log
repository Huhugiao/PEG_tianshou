Epoch #1: 10240it [00:08, 1216.43it/s, env_step=10240, len=151, loss=0.133, loss/clip=-0.006, loss/ent=3.059, loss/vf=0.676, n/ep=3, n/st=256, rew=-79.63]                                                              
Epoch #1: test_reward: -78.954230 ± 12.836301, best_reward: -67.876872 ± 7.825007 in #0
Epoch #2: 10240it [00:07, 1443.07it/s, env_step=20480, len=133, loss=0.150, loss/clip=-0.002, loss/ent=3.145, loss/vf=0.730, n/ep=2, n/st=256, rew=-67.31]                                                              
Epoch #2: test_reward: -62.529343 ± 37.261061, best_reward: -62.529343 ± 37.261061 in #2
Epoch #3: 10240it [00:07, 1435.06it/s, env_step=30720, len=129, loss=0.058, loss/clip=-0.003, loss/ent=3.096, loss/vf=0.371, n/ep=0, n/st=256, rew=-72.58]                                                              
Epoch #3: test_reward: -75.735044 ± 9.163716, best_reward: -62.529343 ± 37.261061 in #2
Epoch #4: 10240it [00:07, 1421.14it/s, env_step=40960, len=137, loss=0.075, loss/clip=-0.003, loss/ent=3.033, loss/vf=0.435, n/ep=3, n/st=256, rew=-72.24]                                                              
Epoch #4: test_reward: -73.655343 ± 5.476678, best_reward: -62.529343 ± 37.261061 in #2
Epoch #5: 10240it [00:07, 1442.26it/s, env_step=51200, len=122, loss=0.067, loss/clip=-0.002, loss/ent=3.101, loss/vf=0.400, n/ep=0, n/st=256, rew=31.37]                                                               
Epoch #5: test_reward: -76.086814 ± 16.888781, best_reward: -62.529343 ± 37.261061 in #2
tracker requires_grad=True, has non-zero gradients=True
target requires_grad=True, has non-zero gradients=False
Training target policy with epoch 10
